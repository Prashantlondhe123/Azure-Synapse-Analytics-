#Azure Synapse Analytics:
Azure Synapse Analytics is a limitless analytics service that brings together data integration, enterprise data warehousing and big data analytics

# Linked service:
In Azure Synapse Analytics, a linked service is where you define your connection information to other services. In this section, you'll add Azure Synapse Analytics and Azure Data Lake Gen 2 as linked services. Open the Synapse Studio and go to the Manage tab. Under External connections, select Linked services.

#Synapse sql:
-Synapse sql is ability to do T sql analytics in synapse analysiytics
-Synapse sql has two consumption model: Dedicated & Server less
-synapse  Pool actually help you to run sql script

# Synapse workspace:
-Synapse Workspace is a collaboration place for doing cloud-based enterprise analytics in Azure
-A Workspace will be associated with ADLS Gen2 and File System
-A workspace allows you to perform analytics with SQL and Apache spark
-Resources available for SQL and Spark analytics are organized into SQL and Spark pools.

#Apache Spark for Synapse:
• This gives ability to do Spark based analytics in Synapse Workspace
• You can create Apache spark pools in Workspace. When you start using that Spark Pool, the workspaces creates spark session to handle the resources associated with that session
• There are two ways to use spark in Synapse
• Spark Notebooks - For doing data science and data engineering using Scala, PySpark, C# and Spark SQL
• Spark Job definitions - For running batch Spark jobs using jar files

#Pipelines: integrate >+>pipeline
• Pipelines are how Azure Synapse provides Data Integration - allowing you to move data between services and orchestrate activities.
• Pipeline are logical grouping of activities that perform a task together.
• Activities defines actions within a Pipeline to perform on data such as copying data, running a Notebook or a SQL script.
. Data flows are a specific kind of activity that provide a no-code experience for doing data transformation

that uses Synapse Spark under-the-covers.

• Trigger - Executes a pipeline. It can be run manually or automatically (schedule, tumbling window or event based)

• Integration dataset - Named view of data that simply points or references the data to be used in an activity as input and output. It belongs to a Linked Service.


#Built in Serverless SQL Pool

• Serverless SQL pools let you use SQL without having to reserve capacity. Billing for a serverless SQL pool is based on the amount of data processed to run the query

Every workspace comes with a pre-configured serverless SQL pool called Built-in.

OPENROWSET() function allows you to access files in Azure Storage and returns the content as a set of rows

The data can be visualized in Synapse Studio by switching from the Table to the Chart view. You can choose among different chart types, such as Area, Bar, Column, Line, Pie, and Scatter.


#Dedicated SQL Pool:

Dedicated SQL Pool - consumes billable resources if its active. You can pause the pool to reduce costs
Your dedicated SQL Pool will be associated with dedicated SQL database

#Create server less Spark Pool:
Apache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.
We can create Server less Spark Pool under Management hub menu in Synapse Studio

#Understand Server less Spark Pool:
-A serverless Spark pool is a way of indicating how a user wants to work with Spark. When you start using a pool a Spark session is created if needed.
-The pool controls how many Spark resources will be used by that session and how long the session will last before it automatically pauses.
- You pay for spark resources used during that session not for the pool itself.
-This is similar to how a serverless SQL pool works. You no need to worry about managing clusters

#Synapse SQL Architecture Components:

• Synapse SQL uses a node-based architecture. Applications connect and issue T-SQL commands to a Control node, which is the single point of entry for Synapse SQL.

• When data is ingested into dedicated SQL pool, the data is sharded into distributions to optimize the performance of the system. You can choose which sharding pattern to use to distribute the data when you define the table. These sharding patterns are supported:

• Hash

• Round Robin

• Replicate

#Compute Node:
• The Compute nodes provide the computational power.
• In dedicated SQL pool, distributions map to Compute nodes for processing.
• In serverless SQL pool, each Compute node is assigned task(small query)
• Data Movement Service (DMS) coordinates data movement between the Compute nodes.

#Distribution:
• A distribution is the basic unit of storage and processing.
• When data is ingested into dedicated SQL pool, the data is sharded into distributions to optimize the performance of the system. You can choose which sharding pattern to use to distribute the data when you define the table. These sharding patterns are supported:
• Hash
• Round Robin
• Replicate

• When dedicated SQL pool runs a query, the work is divided into 60 smaller queries that run in parallel.

#Hash-distributed tables:
• A hash distributed table can deliver the highest query performance for joins and aggregations on large tables.
• Each row belongs to one distribution.
• A deterministic hash algorithm assigns each row to one distribution.
• The number of table rows per distribution varies as shown by the different size of table

#Round-robin distributed tables:
-A round-robin table is the simplest table to create and delivers fast performance when used as a staging table for loads.
-It is quick to load data into a round-robin table, but query performance can often be better with hash distributed tables.

#Replicated tables:
-A replicated table provides the fastest query performance for small tables.

#Serverless SQL Pool:
• Every Azure Synapse Analytics workspace comes with serverless SQL pool endpoints that you can use to query data in the Azure Data Lake (Parquet, Delta Lake, delimited text formats), Cosmos DB, or Dataverse.
• It allows you query data with T-SQL Syntax directly without need to copy or load data into a specialized store.
• Its true Pay as you go service. You will only pay for query execution

#Serverless SQL Pool Benefits:
• Basic discovery and exploration
• Logical data warehouse(LDW)
• Data transformation

#T-SQL Support:
• Serverless SQL pool offers T-SQL querying surface area(LDW).
• Databases-serverless SQL pool endpoint can have multiple databases.
• Schemas - Within a database, there can be one or many object ownership groups called schemas.
• Views, stored procedures, inline table value functions
• External resources - data sources, file formats, and tables
• Serverless SQL pool has no local storage, only metadata objects are stored in databases. Therefore, T-SQL related to the following concepts isn't supported:

• Tables
.Triggers
• Materialized views

. DDL statements other than ones related to views and security
• DML statements

#External Data source
• External data sources are used to establish connectivity with external resources such as Azure Storages. It is primarily used for Data virtualization and data load using Polybase.

CREATE EXTERNAL DATA SOURCE SqlOnDemand Demo WITH LOCATION = "https://sqlondemandstorage.blob.core.windows.net, CREDENTIAL = sqlondemand

#Credential:
• A database scoped credential is a record that contains the authentication information that is required to connect to a resource outside SQL Server.

CREATE DATABASE SCOPED CREDENTIAL sqlondemand
WITH IDENTITY="SHARED ACCESS SIGNATURE',
SECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10
14T12:10:252&se=2061-12 31T12:10:002&sig=KISU2ullCscyTSOAnOnozEpo4t05JAGGBvw/JX2/guw='
Note, Before creating a database scoped credential, the database must have a master key to protect the credential.

#create external data source: csv/Parquet

Use prashant
GO

CREATE MASTER KEY ENCRYPTION BY PASSWORD ='Synapse@123'

CREATE DATABASE SCOPED CREDENTIAL democredential
WITH IDENTITY = 'SHARED ACCESS SIGNATURE',
SECRET='sv=2021-06-08&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2022-08-18T14:08:37Z&st=2022-08-18T06:08:37Z&spr=https&sig=xjp3ENOf0svySd81GPGKcDdO4KFZVKyKgAvUQuan0ME%3D'
GO
CREATE EXTERNAL DATA SOURCE demodatasource WITH (
     LOCATION='https://kkartik1001.dfs.core.windows.net/',
    CREDENTIAL= democredential
);


# create External fileformat:

CREATE EXTERNAL FILE FORMAT ParquetFileFormat
WITH
(    FORMAT_TYPE= PARQUET,
    DATA_COMPRESSION= 'org.apache.hadoop.io.compress.SnappyCodec'
)


#CEATS:
CREATE EXTERNAL TABLE EXTTAXI
WITH
(
LOCATION='/prashant/EXTTAXI.parquet/',
DATA_SOURCE=demodatasource,
FILE_FORMAT=ParquetFileFormat
)
AS 

SELECT
    Locationid,Borough
FROM
    OPENROWSET(
        BULK 'https://kkartik1001.dfs.core.windows.net/prashant/taxidata.parquet/**',
        FORMAT = 'PARQUET'
    ) AS [result]

===>Execute:  select * from EXTTAXI

#CTAS(Create Table as SELECT):
• The CREATE TABLE AS SELECT (CTAS) statement is one of the most important T SQL features available.
• CTAS is a parallel operation that creates a new table based on the output of a SELECT statement. CTAS is the simplest and fastest way to create and insert data into a table with a single command.

#SELECT INTO vs CTAS
-CTAS is a more customizable version of the SELECT...INTO statement.
- SELECT...INTO doesn't allow you to change either the distribution method or the index type as part of the operation.
SELECT...INTO uses the default distribution type of ROUND ROBIN, and the default table structure of CLUSTERED COLUMNSTORE INDEX.
- With CTAS, on the other hand, you can specify both the distribution of the table data as well as the table structure type

CREATE TABLE [dbo].[FactInternetSales_new]
WITH

(

DISTRIBUTION = ROUND_ROBIN ,
 CLUSTERED COLUMNSTORE INDEX
)
AS
SELECT * FROM [dbo].[FactInternetSales];

#1)CREATE TABLE dbo.employeesNew
WITH
(
DISTRIBUTION HASH(empe), 
CLUSTERED COLUMNSTORE INDEX
)
AS
select * from  dbo.employees


#






############## Spark Pool #######
# Parameterize notebook :
Step1: create notebook 1
Step2: create first cell as parameters 
  Name:'Prashant'
Step 3:use that parameters in the 2 nd cell.
Step4:open another notebook 
  %run /notebook2 {'Name':"Sushant"}

#Microsoft Spark Utilities (MSSparkUtils)
• Microsoft Spark Utilities (MSSparkUtils) is a built-in package to help you easily perform common tasks.
• You can use MSSparkUtils to work with file systems, to get environment variables, to chain notebooks together, and to work with secrets.
• MSSparkUtils are available in PySpark(Python), Scala & .NET Spark(c#)

Code> from notebookutils import mssparkutils
      mssparkutils.fs.ls
      mssparkutils.fs.help()
      mssparkutils.

#Environment Utilities
• Environment Utilities helps to get environment details of synapse notebook execution.
mssparkutils.env.help() - To get overview of available methods.
GetUserName(): returns user name 
GetUserId():returns unique user id
 GetJobId(): returns job id 
GetWorkspaceName(): returns workspace name
 GetPoolName(): returns Spark pool name
 GetClusterId(): returns cluster id

Code> mssparkutils.env.GetClusterID()

#Spark Job Definition:
   Spark Jobs lets you run Spark applications on clusters and monitor their status.

#Package Management:
• Libraries provide reusable code that you might want to include in your programs or projects.
• To make third party or locally built code available to your applications, install a library onto one of your serverless Apache Spark pools or notebook session.
• There are 3 levels of package installation.
• Default level
• Spark Pool level
• Session Level
